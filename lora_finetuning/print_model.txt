LoraTraining(
  (inference_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): GPT2LMHeadModel(
        (transformer): GPT2Model(
          (wte): Embedding(50257, 768)
          (wpe): Embedding(1024, 768)
          (drop): Dropout(p=0.1, inplace=False)
          (h): ModuleList(
            (0): GPT2Block(
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): GPT2Attention(
                (c_attn): lora.Linear(
                  (base_layer): Conv1D()
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=768, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=2304, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (c_proj): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (attn_dropout): Dropout(p=0.1, inplace=False)
                (resid_dropout): Dropout(p=0.1, inplace=False)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): GPT2MLP(
                (c_fc): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (c_proj): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (act): NewGELUActivation()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1-11): 11 x GPT2Block(
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): GPT2Attention(
                (c_attn): lora.Linear(
                  (base_layer): CustomLinear(
                    (forward_module): ForwardModuleLinear()
                    (backward_module): BackwardModuleLinear()
                  )
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=768, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=2304, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (c_proj): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (attn_dropout): Dropout(p=0.1, inplace=False)
                (resid_dropout): Dropout(p=0.1, inplace=False)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): GPT2MLP(
                (c_fc): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (c_proj): CustomLinear(
                  (forward_module): ForwardModuleLinear()
                  (backward_module): BackwardModuleLinear()
                )
                (act): NewGELUActivation()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (lm_head): CustomLinear(
          (forward_module): ForwardModuleLinear()
          (backward_module): BackwardModuleLinear()
        )
      )
    )
  )
)
